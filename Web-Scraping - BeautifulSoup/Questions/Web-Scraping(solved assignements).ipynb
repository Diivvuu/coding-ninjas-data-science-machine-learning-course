{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the data of first 3 movies\n",
    "\n",
    "From this link,\n",
    "\n",
    "Find and print the name and genre of the first 3 titles\n",
    "\n",
    "Output Format :\n",
    "\n",
    "title_name_1 ; genre1_1, genre1_2 ..\n",
    "\n",
    "title_name_2 ; genre2_1, genre2_2 ..\n",
    "\n",
    "title_name_2 ; genre3_1, genre3_2 ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BeautifulSoup(response.text, 'html.parser')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War ; Action, Adventure, Sci-Fi\n",
      "Black Panther ; Action, Adventure, Sci-Fi\n",
      "Deadpool 2 ; Action, Adventure, Comedy\n"
     ]
    }
   ],
   "source": [
    "movies = data.find_all(class_ = \"lister-item-content\")\n",
    "    # print(i)\n",
    "movies_list = []\n",
    "genre_list = []\n",
    "for i in movies[:3]:\n",
    "    movies_list.append(i.a.string)\n",
    "    genre_list.append(list(i.find(class_ = \"genre\").stripped_strings)[0])\n",
    "    # print(i.a.string, end = \";\")\n",
    "    # print(list(i.find(class_ = \"genre\").stripped_strings)[0])\n",
    "    # print(end = \";\")\n",
    "for i in range(3):\n",
    "    print(movies_list[i],\";\" , genre_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title with most votes\n",
    "\n",
    "Print the names of movies with highest number of votes from year 2010 to 2014\n",
    "\n",
    "Note : Print the titles line wise starting from year 2010\n",
    "\n",
    "Output Format :\n",
    "\n",
    "title_name_1\n",
    "\n",
    "title_name_2\n",
    "\n",
    "title_name_3\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "Game of Thrones\n",
      "The Dark Knight Rises\n",
      "The Wolf of Wall Street\n",
      "Interstellar\n"
     ]
    }
   ],
   "source": [
    "movies = []\n",
    "for i in range(2010, 2015):\n",
    "    response = requests.get('https://www.imdb.com/search/title/?release_date=' + str(i) + '&sort=num_votes,desc&page=1&ref_=adv_nxt')\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(data.h3.a.string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title with maximum duration\n",
    "\n",
    "Link to use\n",
    "\n",
    "Out of the first 250 titles with highest number of votes in 2018,find which title has the maximum duration.\n",
    "\n",
    "Output Format :\n",
    "\n",
    "title_name title_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Haunting of Hill House 572\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "dict = {}\n",
    "for i in range(1, 250, 50):\n",
    "    response = requests.get('https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=' + str(i) + '&ref_=adv_nxt')\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    tags = data.find_all( class_ = \"lister-item\")\n",
    "    for j in tags:\n",
    "        if j.find( class_ = \"runtime\"):\n",
    "            head = j.find(class_ = \"lister-item-header\")\n",
    "            dur = j.find(class_ = \"runtime\")\n",
    "            t = int(dur.text.strip().split(\" \")[0])\n",
    "            dict[head.a.string] = t\n",
    "maxdur = -1\n",
    "maxname = 0\n",
    "for i,j in dict.items():\n",
    "    if maxdur < j:\n",
    "        maxdur = j\n",
    "        maxname = i\n",
    "print(maxname, maxdur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image with maximum area\n",
    "\n",
    "From this website :\n",
    "\n",
    "Find and print the src of the <img> tag which occupies the maximum area on the page.\n",
    "\n",
    "Note :\n",
    "\n",
    "Ignore images which doesn't have height or width attributes\n",
    "\n",
    "Output Format :\n",
    "\n",
    "src_of_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "response = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "data = BeautifulSoup(response.text,\"html.parser\")\n",
    "image = data.find_all('img')\n",
    "max_area = -1\n",
    "url = ''\n",
    "for i in image:\n",
    "    # if i.has_attr('height') and i.has_attr('width'):\n",
    "    if int(i['height']) * int(i['width']) > max_area:\n",
    "            max_area = int(i['height']) * int(i['width'])\n",
    "            url = i['src']\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quotes with tag humor\n",
    "\n",
    "Find all the quotes that have the tag as \"humor\" from this website\n",
    "\n",
    "Output Format :\n",
    "\n",
    "quote1\n",
    "\n",
    "quote2\n",
    "\n",
    "quote3\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“A day without sunshine is like, you know, night.”\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
      "“Some people never go crazy. What truly horrible lives they must lead.”\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
      "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
      "“I am free of all prejudice. I hate everyone equally. ”\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    response = requests.get('http://quotes.toscrape.com/tag/humor/page/' + str(i) + '/')\n",
    "    data  = BeautifulSoup(response.text, 'html.parser')\n",
    "    # quotes = data.find_all(class_ = \"quote\")\n",
    "    for i in data.select(\".text\"):\n",
    "        print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print all authors\n",
    "\n",
    "Find and print the names of all the different authors from all pages of this website\n",
    "\n",
    "Note : Print the names of all authors line wise sorted in dictionary order\n",
    "\n",
    "Output Format :\n",
    "\n",
    "author1\n",
    "\n",
    "author2\n",
    "\n",
    "author3\n",
    ".\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n",
      "Alexandre Dumas fils\n",
      "Alfred Tennyson\n",
      "Allen Saunders\n",
      "André Gide\n",
      "Ayn Rand\n",
      "Bob Marley\n",
      "C.S. Lewis\n",
      "Charles Bukowski\n",
      "Charles M. Schulz\n",
      "Douglas Adams\n",
      "Dr. Seuss\n",
      "E.E. Cummings\n",
      "Eleanor Roosevelt\n",
      "Elie Wiesel\n",
      "Ernest Hemingway\n",
      "Friedrich Nietzsche\n",
      "Garrison Keillor\n",
      "George Bernard Shaw\n",
      "George Carlin\n",
      "George Eliot\n",
      "George R.R. Martin\n",
      "Harper Lee\n",
      "Haruki Murakami\n",
      "Helen Keller\n",
      "J.D. Salinger\n",
      "J.K. Rowling\n",
      "J.M. Barrie\n",
      "J.R.R. Tolkien\n",
      "James Baldwin\n",
      "Jane Austen\n",
      "Jim Henson\n",
      "Jimi Hendrix\n",
      "John Lennon\n",
      "Jorge Luis Borges\n",
      "Khaled Hosseini\n",
      "Madeleine L'Engle\n",
      "Marilyn Monroe\n",
      "Mark Twain\n",
      "Martin Luther King Jr.\n",
      "Mother Teresa\n",
      "Pablo Neruda\n",
      "Ralph Waldo Emerson\n",
      "Stephenie Meyer\n",
      "Steve Martin\n",
      "Suzanne Collins\n",
      "Terry Pratchett\n",
      "Thomas A. Edison\n",
      "W.C. Fields\n",
      "William Nicholson\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/page/1/')\n",
    "base_url = 'http://quotes.toscrape.com/'\n",
    "authors_list = []\n",
    "while response.status_code == 200:\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    authors = data.find_all(class_ = \"author\")\n",
    "    for i in authors:\n",
    "        if i.string not in authors_list:\n",
    "            authors_list.append(i.string)\n",
    "    if data.find(class_ = \"next\") is None:\n",
    "        break\n",
    "    next = data.find(class_ = \"next\")\n",
    "    next = next.a['href']\n",
    "    next_url = base_url + next\n",
    "    response = requests.get(next_url)\n",
    "for i in sorted(authors_list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birth Date of authors\n",
    "\n",
    "Find the birth date of authors whose name start with 'J' from this website\n",
    "\n",
    "Note : Print a dictionary containing the name as key and the birth date as value.The Names of authors should be alphabetically sorted.\n",
    "\n",
    "Output Format :\n",
    "\n",
    "{'author_1': 'month day, year', 'author_2': 'month day, year', ....}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J.D. Salinger': 'January 01, 1919', 'J.K. Rowling': 'July 31, 1965', 'J.M. Barrie': 'May 09, 1860', 'J.R.R. Tolkien': 'January 03, 1892', 'James Baldwin': 'August 02, 1924', 'Jane Austen': 'December 16, 1775', 'Jim Henson': 'September 24, 1936', 'Jimi Hendrix': 'November 27, 1942', 'John Lennon': 'October 09, 1940', 'Jorge Luis Borges': 'August 24, 1899'}\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://quotes.toscrape.com'\n",
    "dct = {}\n",
    "# while response.status_code == 200:\n",
    "for i in range(1, 11):\n",
    "    response = requests.get('http://quotes.toscrape.com/page/' + str(i) + '/')\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    for i in data.select(\".author\"):\n",
    "        if i.text[0] == 'J':\n",
    "            dct[i.text] = i.next_sibling.next_sibling['href']\n",
    "            about_url = i.next_sibling.next_sibling['href']\n",
    "    # response = requests.get(base_url + about_url)\n",
    "bdate = {}\n",
    "for authors in sorted(dct):\n",
    "    response = requests.get(base_url + dct[authors])\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    date = data.find(class_ = \"author-born-date\")\n",
    "    bdate[authors] = date.string\n",
    "print(bdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quotes by Albert Einstein\n",
    "\n",
    "Find all the quotes by Albert Einstein(in the order they appear on the page) from this website\n",
    "\n",
    "Note : Fetch data from all the pages.\n",
    "\n",
    "Output Format :\n",
    "\n",
    "quote1\n",
    "\n",
    "quote2\n",
    "\n",
    "quote3\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidURL",
     "evalue": "Failed to parse: http://quotes.toscrape.com\nNext →\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocationParseError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:434\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 434\u001b[0m     scheme, auth, host, port, path, query, fragment \u001b[39m=\u001b[39m parse_url(url)\n\u001b[0;32m    435\u001b[0m \u001b[39mexcept\u001b[39;00m LocationParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\url.py:397\u001b[0m, in \u001b[0;36mparse_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m):\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39;49mraise_from(LocationParseError(source_url), \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    399\u001b[0m \u001b[39m# For the sake of backwards compatibility we put empty\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m# string values for path if there are any defined values\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m# beyond the path in the URL.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39m# TODO: Remove this when we break backwards compatibility.\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mLocationParseError\u001b[0m: Failed to parse: http://quotes.toscrape.com\nNext →\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m next_url \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mfind(class_ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m next_url \u001b[39m=\u001b[39m next_url\u001b[39m.\u001b[39mtext\n\u001b[1;32m----> 9\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(base_url \u001b[39m+\u001b[39;49m next_url)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mstatus_code)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:436\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    434\u001b[0m     scheme, auth, host, port, path, query, fragment \u001b[39m=\u001b[39m parse_url(url)\n\u001b[0;32m    435\u001b[0m \u001b[39mexcept\u001b[39;00m LocationParseError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 436\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[0;32m    439\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant http://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n",
      "\u001b[1;31mInvalidURL\u001b[0m: Failed to parse: http://quotes.toscrape.com\nNext →\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get('http://quotes.toscrape.com')\n",
    "base_url = 'http://quotes.toscrape.com'\n",
    "while response.status_code == 200:\n",
    "    data = BeautifulSoup(response.text, 'html.parser')\n",
    "    next_url = data.find(class_ = \"next\")\n",
    "    next_url = next_url.text\n",
    "    response = requests.get(base_url + next_url)\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
